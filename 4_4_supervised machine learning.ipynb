{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "877a0748",
   "metadata": {},
   "source": [
    "### **Supervised Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656bf74c",
   "metadata": {},
   "source": [
    "We will split the steps in doing supervised machine learning into few parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba25336a",
   "metadata": {},
   "source": [
    "#### Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b446253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    recall_score, precision_score, accuracy_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f13d0d",
   "metadata": {},
   "source": [
    "#### Reading dataset and selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f8e0b607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ACCIDENT_NO', 'ACCIDENT_DATE', 'ACCIDENT_TIME', 'LIGHT_CONDITION',\n",
       "       'ROAD_GEOMETRY', 'ROAD_GEOMETRY_DESC', 'SEVERITY', 'SPEED_ZONE',\n",
       "       'ATMOSPH_COND', 'ATMOSPH_COND_DESC', 'SURFACE_COND',\n",
       "       'SURFACE_COND_DESC', 'ACCIDENT_YEAR', 'VEHICLE_AGE',\n",
       "       'TOTAL_NO_OCCUPANTS', 'VEHICLE_TYPE', 'TRAFFIC_CONTROL',\n",
       "       'ROAD_SURFACE_TYPE', 'AGE_GROUP', 'SEATING_POSITION',\n",
       "       'HELMET_BELT_WORN', 'SEVERITY_ORD', 'AGE_ORD', 'SEAT_CATEGORY',\n",
       "       'VEHICLE_TYPE_CAT', 'TIME_OF_DAY', 'SEVERITY_BINARY',\n",
       "       'SURFACE_COND_BINARY', 'ATMOSPH_COND_BINARY', 'SEAT_BINARY',\n",
       "       'HELMET_BELT_BINARY', 'TRAFFIC_CONTROL_BINARY', 'ROAD_GEOMETRY_BINARY',\n",
       "       'LIGHT_CONDITION_BINARY', 'SPEED_ZONE_BINARY', 'VEHICLE_TYPE_BINARY',\n",
       "       'SURFACE_COND_RISK', 'ATMOSPH_COND_RISK', 'WEATHER_RISK',\n",
       "       'VEHICLE_AGE_NORM', 'TOTAL_OCCUPANTS_NORM'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAccident = pd.read_csv('datasets/accident_clean.csv')\n",
    "dfAccident.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b9009475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining categorical features with some can be treated as binary\n",
    "features_binary = [\n",
    "    'LIGHT_CONDITION_BINARY', 'ROAD_GEOMETRY',\n",
    "    'TRAFFIC_CONTROL_BINARY', 'HELMET_BELT_BINARY',\n",
    "    'VEHICLE_TYPE_BINARY','SEAT_BINARY',\n",
    "    'SPEED_ZONE_BINARY', 'TIME_OF_DAY','AGE_ORD',\n",
    "    # 'ATMOSPH_COND_BINARY','SURFACE_COND_BINARY'\n",
    "    ]\n",
    "X_bin = dfAccident[features_binary].astype('category')\n",
    "\n",
    "# Defining categorical features\n",
    "features_category = [\n",
    "    'LIGHT_CONDITION', 'ROAD_GEOMETRY',\n",
    "    'TRAFFIC_CONTROL', 'HELMET_BELT_WORN',\n",
    "    'VEHICLE_TYPE_CAT','SEAT_CATEGORY',\n",
    "    'SPEED_ZONE', 'TIME_OF_DAY',\n",
    "    # 'ATMOSPH_COND','SURFACE_COND'\n",
    "    ]\n",
    "X_cat = dfAccident[features_category].astype('category')\n",
    "\n",
    "# Defining numerical features\n",
    "X_num = ['TOTAL_OCCUPANTS_NORM',\n",
    "         'WEATHER_RISK',\n",
    "         'VEHICLE_AGE_NORM']\n",
    "\n",
    "# Defining target features\n",
    "y = dfAccident['SEVERITY_BINARY']\n",
    "\n",
    "# One hot features encoding\n",
    "X_encoded_bin = pd.get_dummies(X_bin, drop_first=True)\n",
    "X_encoded_cat = pd.get_dummies(X_cat, drop_first=True)\n",
    "\n",
    "# Merging dataset\n",
    "X_bin_original = pd.concat([X_bin, dfAccident[X_num]], axis=1)\n",
    "X_bin_one_hot = pd.concat([X_encoded_bin, dfAccident[X_num]], axis=1)\n",
    "X_cat_original = pd.concat([X_cat, dfAccident[X_num]], axis=1)\n",
    "X_cat_one_hot = pd.concat([X_encoded_cat, dfAccident[X_num]], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f580ebaf",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "09ed1f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_model (X_type, name_type):\n",
    "    # Training test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_type, y, test_size=0.3, random_state=1)\n",
    "\n",
    "    # Train logistic regression\n",
    "    model_logit = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "    model_logit.fit(X_train, y_train)\n",
    "\n",
    "    # Making prediction of y\n",
    "    y_pred = model_logit.predict(X_test)\n",
    "    y_prob = model_logit.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Testing AUC and accuracy of model\n",
    "    auc = round(roc_auc_score(y_test, y_prob),4)\n",
    "    acc = round(accuracy_score(y_test, y_pred),4)\n",
    "\n",
    "    print(\"Logit Model\", name_type , \"AUC is \",auc)\n",
    "    print(\"Logit Model\", name_type , \"Accuracy is \",acc)\n",
    "    print(\"Logit Model\", name_type , \"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print()\n",
    "    return model_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "10803e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit Model Categorical Original AUC is  0.7532\n",
      "Logit Model Categorical Original Accuracy is  0.725\n",
      "Logit Model Categorical Original Confusion Matrix:\n",
      " [[21866  8260]\n",
      " [  181   384]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.73      0.84     30126\n",
      "           1       0.04      0.68      0.08       565\n",
      "\n",
      "    accuracy                           0.72     30691\n",
      "   macro avg       0.52      0.70      0.46     30691\n",
      "weighted avg       0.97      0.72      0.82     30691\n",
      "\n",
      "\n",
      "Logit Model Categorical One-Hot AUC is  0.7656\n",
      "Logit Model Categorical One-Hot Accuracy is  0.7474\n",
      "Logit Model Categorical One-Hot Confusion Matrix:\n",
      " [[22559  7567]\n",
      " [  186   379]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.75      0.85     30126\n",
      "           1       0.05      0.67      0.09       565\n",
      "\n",
      "    accuracy                           0.75     30691\n",
      "   macro avg       0.52      0.71      0.47     30691\n",
      "weighted avg       0.97      0.75      0.84     30691\n",
      "\n",
      "\n",
      "Logit Model Binary Original AUC is  0.7642\n",
      "Logit Model Binary Original Accuracy is  0.6791\n",
      "Logit Model Binary Original Confusion Matrix:\n",
      " [[20426  9700]\n",
      " [  149   416]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.68      0.81     30126\n",
      "           1       0.04      0.74      0.08       565\n",
      "\n",
      "    accuracy                           0.68     30691\n",
      "   macro avg       0.52      0.71      0.44     30691\n",
      "weighted avg       0.98      0.68      0.79     30691\n",
      "\n",
      "\n",
      "Logit Model Binary One-Hot AUC is  0.7658\n",
      "Logit Model Binary One-Hot Accuracy is  0.6726\n",
      "Logit Model Binary One-Hot Confusion Matrix:\n",
      " [[20223  9903]\n",
      " [  146   419]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.67      0.80     30126\n",
      "           1       0.04      0.74      0.08       565\n",
      "\n",
      "    accuracy                           0.67     30691\n",
      "   macro avg       0.52      0.71      0.44     30691\n",
      "weighted avg       0.98      0.67      0.79     30691\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_cat_original = logistic_model(X_cat_original, \"Categorical Original\")\n",
    "model_cat_one_hot = logistic_model(X_cat_one_hot, \"Categorical One-Hot\")\n",
    "model_bin_original = logistic_model(X_bin_original, \"Binary Original\")\n",
    "model_bin_one_hot = logistic_model(X_bin_one_hot, \"Binary One-Hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2e6b5829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature importance\n",
    "# importance = pd.Series(model_logit.coef_[0], index=X_type.columns)\n",
    "# importance = importance.abs().sort_values(ascending=False)\n",
    "\n",
    "# print(f\"\\nTop 10 Feature Importances for {name_type}:\\n{importance.head(5)}\")\n",
    "# print()\n",
    "\n",
    "#     # SHAP explanation\n",
    "# print(f\"\\nGenerating SHAP summary plot for {name_type}...\")\n",
    "# explainer = shap.Explainer(model_logit, X_train)\n",
    "# shap_values = explainer(X_test)\n",
    "# shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e45df5",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0bbcbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree (X_type, name_type):\n",
    "    \n",
    "    # Training test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_type, y, test_size=0.3, random_state=1)\n",
    "    \n",
    "    # Train decision tree modelling\n",
    "    dtree = DecisionTreeClassifier(random_state=30, max_depth=5, min_samples_leaf=30, class_weight='balanced')\n",
    "    dtree.fit(X_train, y_train)\n",
    "    \n",
    "    # Making prediction of y using decision tree\n",
    "    y_pred = dtree.predict(X_test)\n",
    "    y_prob = dtree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Testing AUC and accuracy of model\n",
    "    auc = round(roc_auc_score(y_test, y_prob),4)\n",
    "    acc = round(accuracy_score(y_test, y_pred),4)\n",
    "\n",
    "    print(\"Decision Tree\", name_type , \"AUC is \",auc)\n",
    "    print(\"Decision Tree\", name_type , \"Accuracy is \",acc)\n",
    "    print(\"Decision Tree\", name_type , \"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print()\n",
    "    return dtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "84e43759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Categorical Original AUC is  0.7399\n",
      "Decision Tree Categorical Original Accuracy is  0.7707\n",
      "Decision Tree Categorical Original Confusion Matrix:\n",
      " [[23307  6819]\n",
      " [  219   346]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.77      0.87     30126\n",
      "           1       0.05      0.61      0.09       565\n",
      "\n",
      "    accuracy                           0.77     30691\n",
      "   macro avg       0.52      0.69      0.48     30691\n",
      "weighted avg       0.97      0.77      0.85     30691\n",
      "\n",
      "\n",
      "Decision Tree Categorical One-Hot AUC is  0.701\n",
      "Decision Tree Categorical One-Hot Accuracy is  0.7943\n",
      "Decision Tree Categorical One-Hot Confusion Matrix:\n",
      " [[24059  6067]\n",
      " [  246   319]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.80      0.88     30126\n",
      "           1       0.05      0.56      0.09       565\n",
      "\n",
      "    accuracy                           0.79     30691\n",
      "   macro avg       0.52      0.68      0.49     30691\n",
      "weighted avg       0.97      0.79      0.87     30691\n",
      "\n",
      "\n",
      "Decision Tree Binary Original AUC is  0.7441\n",
      "Decision Tree Binary Original Accuracy is  0.6097\n",
      "Decision Tree Binary Original Confusion Matrix:\n",
      " [[18254 11872]\n",
      " [  106   459]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.75     30126\n",
      "           1       0.04      0.81      0.07       565\n",
      "\n",
      "    accuracy                           0.61     30691\n",
      "   macro avg       0.52      0.71      0.41     30691\n",
      "weighted avg       0.98      0.61      0.74     30691\n",
      "\n",
      "\n",
      "Decision Tree Binary One-Hot AUC is  0.7378\n",
      "Decision Tree Binary One-Hot Accuracy is  0.6069\n",
      "Decision Tree Binary One-Hot Confusion Matrix:\n",
      " [[18169 11957]\n",
      " [  107   458]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.60      0.75     30126\n",
      "           1       0.04      0.81      0.07       565\n",
      "\n",
      "    accuracy                           0.61     30691\n",
      "   macro avg       0.52      0.71      0.41     30691\n",
      "weighted avg       0.98      0.61      0.74     30691\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtree_cat_original = decision_tree(X_cat_original, \"Categorical Original\")\n",
    "dtree_cat_one_hot = decision_tree(X_cat_one_hot, \"Categorical One-Hot\")\n",
    "dtree_bin_original = decision_tree(X_bin_original, \"Binary Original\")\n",
    "dtree_bin_one_hot = decision_tree(X_bin_one_hot, \"Binary One-Hot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17429667",
   "metadata": {},
   "source": [
    "#### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfcb043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def light_gbm (X_type, name_type):\n",
    "    \n",
    "    # Training test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_type, y, test_size=0.3, random_state=1)\n",
    "    \n",
    "    # Calculate scale_pos_weight for class imbalance\n",
    "    scale_pos_weight = (len(y_train) - sum(y_train)) / sum(y_train)\n",
    "    \n",
    "    # Train LightGBM model\n",
    "    lgb = LGBMClassifier(scale_pos_weight=scale_pos_weight,\n",
    "                         max_depth=6, min_samples_leaf=15,\n",
    "                         verbose=-1,\n",
    "                         random_state=42)\n",
    "    lgb.fit(X_train, y_train)\n",
    "    \n",
    "    # Making prediction of y using decision tree\n",
    "    y_pred = lgb.predict(X_test)\n",
    "    y_prob = lgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Testing AUC and accuracy of model\n",
    "    auc = round(roc_auc_score(y_test, y_prob),4)\n",
    "    acc = round(accuracy_score(y_test, y_pred),4)\n",
    "\n",
    "    print(\"LightGBM\", name_type , \"AUC is \",auc)\n",
    "    print(\"LightGBM\", name_type , \"Accuracy is \",acc)\n",
    "    print(\"LightGBM\", name_type , \"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print()\n",
    "    return lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "de9e0417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Categorical Original AUC is  0.7457\n",
      "LightGBM Categorical Original Accuracy is  0.7733\n",
      "LightGBM Categorical Original Confusion Matrix:\n",
      " [[23384  6742]\n",
      " [  217   348]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.78      0.87     30126\n",
      "           1       0.05      0.62      0.09       565\n",
      "\n",
      "    accuracy                           0.77     30691\n",
      "   macro avg       0.52      0.70      0.48     30691\n",
      "weighted avg       0.97      0.77      0.86     30691\n",
      "\n",
      "\n",
      "LightGBM Categorical One-Hot AUC is  0.7487\n",
      "LightGBM Categorical One-Hot Accuracy is  0.7757\n",
      "LightGBM Categorical One-Hot Confusion Matrix:\n",
      " [[23463  6663]\n",
      " [  221   344]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.78      0.87     30126\n",
      "           1       0.05      0.61      0.09       565\n",
      "\n",
      "    accuracy                           0.78     30691\n",
      "   macro avg       0.52      0.69      0.48     30691\n",
      "weighted avg       0.97      0.78      0.86     30691\n",
      "\n",
      "\n",
      "LightGBM Binary Original AUC is  0.7486\n",
      "LightGBM Binary Original Accuracy is  0.7192\n",
      "LightGBM Binary Original Confusion Matrix:\n",
      " [[21711  8415]\n",
      " [  204   361]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.72      0.83     30126\n",
      "           1       0.04      0.64      0.08       565\n",
      "\n",
      "    accuracy                           0.72     30691\n",
      "   macro avg       0.52      0.68      0.46     30691\n",
      "weighted avg       0.97      0.72      0.82     30691\n",
      "\n",
      "\n",
      "LightGBM Binary One-Hot AUC is  0.7444\n",
      "LightGBM Binary One-Hot Accuracy is  0.7103\n",
      "LightGBM Binary One-Hot Confusion Matrix:\n",
      " [[21424  8702]\n",
      " [  189   376]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.71      0.83     30126\n",
      "           1       0.04      0.67      0.08       565\n",
      "\n",
      "    accuracy                           0.71     30691\n",
      "   macro avg       0.52      0.69      0.45     30691\n",
      "weighted avg       0.97      0.71      0.81     30691\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgb_cat_original = light_gbm(X_cat_original, \"Categorical Original\")\n",
    "lgb_cat_one_hot = light_gbm(X_cat_one_hot, \"Categorical One-Hot\")\n",
    "lgb_bin_original = light_gbm(X_bin_original, \"Binary Original\")\n",
    "lgb_bin_one_hot = light_gbm(X_bin_one_hot, \"Binary One-Hot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedcf7d3",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "44ac158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost (X_type, name_type):\n",
    "    \n",
    "    # Training test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_type, y, test_size=0.3, random_state=1)\n",
    "    \n",
    "    # Calculate scale_pos_weight for class imbalance\n",
    "    scale_pos_weight = (len(y_train) - sum(y_train)) / sum(y_train)\n",
    "    \n",
    "    # Train LightGBM model\n",
    "    xgb = XGBClassifier(scale_pos_weight=scale_pos_weight,\n",
    "                        # n_estimators=200, learning_rate=0.05, max_depth=6,\n",
    "                        random_state=42,\n",
    "                        eval_metric='auc'\n",
    "                        )\n",
    "    \n",
    "    xgb.fit(X_train, y_train)\n",
    "    \n",
    "    # Making prediction of y using decision tree\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    y_prob = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Testing AUC and accuracy of model\n",
    "    auc = round(roc_auc_score(y_test, y_prob),4)\n",
    "    acc = round(accuracy_score(y_test, y_pred),4)\n",
    "\n",
    "    print(\"XGBoost\", name_type , \"AUC is \",auc)\n",
    "    print(\"XGBoost\", name_type , \"Accuracy is \",acc)\n",
    "    print(\"XGBoost\", name_type , \"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print()\n",
    "    return xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "01ec5622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Categorical One-Hot AUC is  0.7186\n",
      "XGBoost Categorical One-Hot Accuracy is  0.7949\n",
      "XGBoost Categorical One-Hot Confusion Matrix:\n",
      " [[24089  6037]\n",
      " [  258   307]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.80      0.88     30126\n",
      "           1       0.05      0.54      0.09       565\n",
      "\n",
      "    accuracy                           0.79     30691\n",
      "   macro avg       0.52      0.67      0.49     30691\n",
      "weighted avg       0.97      0.79      0.87     30691\n",
      "\n",
      "\n",
      "XGBoost Binary One-Hot AUC is  0.7022\n",
      "XGBoost Binary One-Hot Accuracy is  0.7976\n",
      "XGBoost Binary One-Hot Confusion Matrix:\n",
      " [[24202  5924]\n",
      " [  288   277]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.80      0.89     30126\n",
      "           1       0.04      0.49      0.08       565\n",
      "\n",
      "    accuracy                           0.80     30691\n",
      "   macro avg       0.52      0.65      0.48     30691\n",
      "weighted avg       0.97      0.80      0.87     30691\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_cat_one_hot = xgboost(X_cat_one_hot, \"Categorical One-Hot\")\n",
    "xgb_bin_one_hot = xgboost(X_bin_one_hot, \"Binary One-Hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2dd55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
